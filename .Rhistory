data <- read.csv("outcome-of-care-measures.csv", colClasses = "character")
#create a list of states and initialize a character array to hold the
#required hospital names
state <- levels(factor(data[, 7]))
hospital <- vector(mode="character")
for (i in seq(state)) {
hospital[i] <- rankhospital(state[i], outcome, num)
}
data.frame(hospital, state)
}
head(rankall("heart attack", 20), 10)
tail(rankall("pneumonia", "worst"), 3)
tail(rankall("heart failure"), 10)
source("http://d396qusza40orc.cloudfront.net/rprog%2Fscripts%2Fsubmitscript3.R")
source("submitscript3.R")
submit()manual = y
submit()manual = 1
submit()
2
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
getwd()
DT <- fread(input="getdata-data-ss06pid.csv", sep=",")
library(data.table)
install.packages(data;table)
install.packages(data.table)
install.packages("data.table")
library(data.table)
DT <- fread(input="getdata-data-ss06pid.csv", sep=",")
getwd()
DT <- fread(input="getdata_data_ss06pid.csv", sep=",")
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
install.packages(httr)
install.packages("httr")
library(httr)
# 1. Find OAuth settings for github:
#    http://developer.github.com/v3/oauth/
oauth_endpoints("github")
# 2. To make your own application, register at at
#    https://github.com/settings/applications. Use any URL for the homepage URL
#    (http://github.com is fine) and  http://localhost:1410 as the callback url
#
#    Replace your key and secret below.
myapp <- oauth_app("github",
key = "911b684b0ee7a127c5d4",
secret = "203db28e353b9489e1e591054909a8e8c41ff54c")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
oauth_endpoints("github")
myapp <- oauth_app("github",
key = "911b684b0ee7a127c5d4",
secret = "203db28e353b9489e1e591054909a8e8c41ff54c")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
exit
stop()
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github",
key = "911b684b0ee7a127c5d4",
secret = "203db28e353b9489e1e591054909a8e8c41ff54c")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
install.packages("httpuv"")
install.packages("httpuv")
install.packages("httpuv")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
content(req)
BROWSE("https://api.github.com/users/jtleek/repos",authenticate("Access Token","x-oauth-basic","basic"))
BROWSE("https://api.github.com/users/jtleek/repos",authenticate("Access Token","x-oauth-basic","basic"))
library(sqldf)
install.packages("sqldf"")
install.packages("sqldf"")
install.packages("sqldf")
library(sqldf)
getwd()
acs <- read.csv("./getdata-data-ss06pid.csv", header=T, sep=",")
acs <- read.csv("getdata-data-ss06pid.csv", header=T, sep=",")
getwd()
acs <- read.csv("./getdata_data_ss06pid.csv", header=T, sep=",")
head(acs)
sqldf("select pwgtp1 from acs where AGEP < 50")
acs$AGEP
length(unique(acs$AGEP))
length(unique(sqldf("select distinct AGEP from acs")))
sqldf("select distinct AGEP from acs")
sqldf("select unique AGEP from acs")
sqldf("select distinct pwgtp1 from acs")
sqldf("select AGEP where unique from acs")
connection <- url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode <- readLines(connection)
close(connection)
c(nchar(htmlCode[10]), nchar(htmlCode[20]), nchar(htmlCode[30]), nchar(htmlCode[100]))
require(httr)
require(XML)
htmlCode <- GET("http://biostat.jhsph.edu/~jleek/contact.html")
content <- content(htmlCode, as="text")
htmlParsed <- htmlParse(content, asText=TRUE)
xpathSApply(htmlParsed, "//title", xmlValue)
install.packages("xml")
install.packages("XML")
require(httr)
require(XML)
htmlCode <- GET("http://biostat.jhsph.edu/~jleek/contact.html")
content <- content(htmlCode, as="text")
htmlParsed <- htmlParse(content, asText=TRUE)
xpathSApply(htmlParsed, "//title", xmlValue)
list.files()
data <- read.csv("./getdata-wksst8110.for", header=T)
data <- read.csv("./getdata_wksst8110.for", header=T)
head(data)
dim(data)
file_name <- "./getdata-wksst8110.for"
df <- read.fwf(file=file_name,widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
head(df)
sum(df[, 4])
file_name <- "./getdata_wksst8110.for"
df <- read.fwf(file=file_name,widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
head(df)
sum(df[, 4])
d[, grep("^[^filler]", names(d))]
d <- read.fwf(url, w, header=FALSE, skip=4, col.names=colNames)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
lines <- readLines(url, n=10)
w <- c(1, 9, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3)
colNames <- c("filler", "week", "filler", "sstNino12", "filler", "sstaNino12", "filler", "sstNino3", "filler", "sstaNino3", "filler", "sstNino34", "filler", "sstaNino34", "filler", "sstNino4", "filler", "sstaNino4")
d <- read.fwf(url, w, header=FALSE, skip=4, col.names=colNames)
d <- d[, grep("^[^filler]", names(d))]
sum(d[, 4])
library(sqldf)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile = "acs.csv", method = "curl")
dateDownloaded <- date()
acs <- read.table("./acs.csv",sep=",",header=TRUE)
head(acs)
# households on greater than 10 acres who sold more than $10,000 worth of agriculture products
# ACR=3 AND AGS=6
agricultureLogical<-(acs$ACR==3 & acs$AGS==6)
which(agricultureLogical)
class(agricultureLogical)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile = "acs.csv", method = "curl")
file.dest <- 'GDP.csv'
download.file(fileUrl, file.dest )
acs <- read.table("./GDP.csv",sep=",",header=TRUE)
head(acs)
agricultureLogical<-(acs$ACR==3 & acs$AGS==6)
which(agricultureLogical)
class(agricultureLogical)
install.packages("jpeg")
library(jpeg)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
download.file(fileUrl, destfile = "jeff.jpg", method = "curl")
img.n<-readJPEG("jeff.jpg",TRUE)
quantile(img.n,probs=c(0.3,0.8))
download.file(fileUrl, destfile = "jeff.jpg")
img.n<-readJPEG("jeff.jpg",TRUE)
quantile(img.n,probs=c(0.3,0.8))
quantile(img.n,probs=c(0.3,0.8))
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(fileUrl, destfile = "gdp.csv")
gdp <- read.csv("./gdp.csv")
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(fileUrl1, destfile = "edu.csv")
edu <- read.csv("./edu.csv")
X=CountryCode
names(gdp)
names(edu)
head(gdp)
head(edu)
gdpclean<-gdp[5:194,]
mergedData=as.data.frame(merge(gdpclean,edu,by.x="X",by.y="CountryCode"))
mergedData$Gross.domestic.product.2012 = as.numeric(as.character(mergedData$Gross.domestic.product.2012))
summary(mergedData[mergedData$Income.Group=="High income: OECD",])
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv'
file.dest <- 'GDP.csv'
# download from the URL
download.file(file.url, file.dest )
# specify the right lines
rowNames <- seq(10,200, 2)
# read the data
gdp <- read.csv('GDP.csv', header=F, skip=5, nrows=190)
View(gdp)
# second data file
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv'
file.dest <- 'GDP2.csv'
# download from the URL
download.file(file.url, file.dest )
# read second file
fed <- read.csv('GDP2.csv')
View(fed)
# merge datasets
combined <- merge(gdp, fed, by.x='V1', by.y='CountryCode', sort=TRUE)
View(combined)
mean(combined[combined$Income.Group=='High income: OECD',]$V2)
# non OECD
mean(combined[combined$Income.Group=='High income: nonOECD',]$V2)
quentile <- c(0.2,0.4,0.6,0.8,1)
q <- quantile(combined$V2, quentile)
q1 <- combined$V2 <= 38
xtabs(q1 ~ combined$Income.Group)
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg'
file.dest <- 'jeff.jpg'
# download from the URL
download.file(file.url, file.dest, mode='wb' )
# load package
library(jpeg)
# load the data
picture <- readJPEG('jeff.jpg', native=TRUE)
# get the quantile info
quantile(picture, probs = c(0.3, 0.8) )
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv'
file.dest <- 'GDP.csv'
# download from the URL
download.file(file.url, file.dest )
# specify the right lines
rowNames <- seq(10,200, 2)
# read the data
gdp <- read.csv('GDP.csv', header=F, skip=5, nrows=190)
View(gdp)
# second data file
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv'
file.dest <- 'GDP2.csv'
# download from the URL
download.file(file.url, file.dest )
# read second file
fed <- read.csv('GDP2.csv')
View(fed)
# merge datasets
combined <- merge(gdp, fed, by.x='V1', by.y='CountryCode', sort=TRUE)
View(combined)
# Q3.
# sort the data
combined[with(combined, order(-V2) )]
file.url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv'
file.dest <- 'ACS.csv'
# download from the URL
download.file(file.url, file.dest)
# read the data
ACS <- read.csv('ACS.csv')
# create vector
ACS$agricultureLogical <- ifelse(ACS$ACR==3 & ACS$AGS==6,TRUE,FALSE)
# read lines
which(ACS$agricultureLogical)
library(plyr)
library(reshape2)
library(plyr)
library(reshape2)
filename <- "getdata_projectfiles_UCI HAR Dataset.zip"
## Download and unzip the dataset:
if (!file.exists(filename)) {
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip "
download.file(fileURL, filename)
}
if (!file.exists("UCI HAR Dataset")) {
unzip(filename)
}
# Load activity labels + features
activityLabels <- read.table("UCI HAR Dataset/activity_labels.txt")
activityLabels[,2] <- as.character(activityLabels[,2])
features <- read.table("UCI HAR Dataset/features.txt")
features[,2] <- as.character(features[,2])
# Extract only the data on mean and standard deviation
featuresWanted <- grep(".*mean.*|.*std.*", features[,2])
featuresWanted.names <- features[featuresWanted,2]
featuresWanted.names = gsub('-mean', 'Mean', featuresWanted.names)
featuresWanted.names = gsub('-std', 'Std', featuresWanted.names)
featuresWanted.names <- gsub('[-()]', '', featuresWanted.names)
# Load the datasets
train <- read.table("UCI HAR Dataset/train/X_train.txt")[featuresWanted]
trainActivities <- read.table("UCI HAR Dataset/train/Y_train.txt")
trainSubjects <- read.table("UCI HAR Dataset/train/subject_train.txt")
train <- cbind(trainSubjects, trainActivities, train)
test <- read.table("UCI HAR Dataset/test/X_test.txt")[featuresWanted]
testActivities <- read.table("UCI HAR Dataset/test/Y_test.txt")
testSubjects <- read.table("UCI HAR Dataset/test/subject_test.txt")
test <- cbind(testSubjects, testActivities, test)
# merge datasets and add labels
allData <- rbind(train, test)
colnames(allData) <- c("subject", "activity", featuresWanted.names)
# turn activities & subjects into factors
allData$activity <- factor(allData$activity, levels = activityLabels[,1], labels = activityLabels[,2])
allData$subject <- as.factor(allData$subject)
allData.melted <- melt(allData, id = c("subject", "activity"))
allData.mean <- dcast(allData.melted, subject + activity ~ variable, mean)
write.table(allData.mean, "tidy.txt", row.names = FALSE, quote = FALSE)
unzip(filename)
if (!file.exists(filename)) {
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip "
download.file(fileURL, filename, mode='wb', cacheOK=FALSE)
}
if (!file.exists("UCI HAR Dataset")) {
unzip(filename)
exit
filename <- "getdata_projectfiles_UCI HAR Dataset.zip"
## Download and unzip the dataset:
if (!file.exists(filename)) {
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip "
download.file(fileURL, filename, mode='wb', cacheOK=FALSE)
library(plyr)
library(reshape2)
# Load activity labels + features
activityLabels <- read.table("UCI HAR Dataset/activity_labels.txt")
activityLabels[,2] <- as.character(activityLabels[,2])
features <- read.table("UCI HAR Dataset/features.txt")
features[,2] <- as.character(features[,2])
# Extract only the data on mean and standard deviation
featuresWanted <- grep(".*mean.*|.*std.*", features[,2])
featuresWanted.names <- features[featuresWanted,2]
featuresWanted.names = gsub('-mean', 'Mean', featuresWanted.names)
featuresWanted.names = gsub('-std', 'Std', featuresWanted.names)
featuresWanted.names <- gsub('[-()]', '', featuresWanted.names)
# Load the datasets
train <- read.table("UCI HAR Dataset/train/X_train.txt")[featuresWanted]
trainActivities <- read.table("UCI HAR Dataset/train/Y_train.txt")
trainSubjects <- read.table("UCI HAR Dataset/train/subject_train.txt")
train <- cbind(trainSubjects, trainActivities, train)
test <- read.table("UCI HAR Dataset/test/X_test.txt")[featuresWanted]
testActivities <- read.table("UCI HAR Dataset/test/Y_test.txt")
testSubjects <- read.table("UCI HAR Dataset/test/subject_test.txt")
test <- cbind(testSubjects, testActivities, test)
# merge datasets and add labels
allData <- rbind(train, test)
colnames(allData) <- c("subject", "activity", featuresWanted.names)
# turn activities & subjects into factors
allData$activity <- factor(allData$activity, levels = activityLabels[,1], labels = activityLabels[,2])
allData$subject <- as.factor(allData$subject)
allData.melted <- melt(allData, id = c("subject", "activity"))
allData.mean <- dcast(allData.melted, subject + activity ~ variable, mean)
write.table(allData.mean, "tidy.txt", row.names = FALSE, quote = FALSE)
averageData<-aggregate(. ~subject + activity, allData, mean)
averageData<-averageData[order(averageData$subject,averageData$activity),]
write.table(averageData, file = "averagetidydata.txt",row.name=FALSE)
library(reshape2)
library(plyr)
# Load activity labels + features
activityLabels <- read.table("UCI HAR Dataset/activity_labels.txt")
activityLabels[,2] <- as.character(activityLabels[,2])
features <- read.table("UCI HAR Dataset/features.txt")
features[,2] <- as.character(features[,2])
# Extract only the data on mean and standard deviation
featuresWanted <- grep(".*mean.*|.*std.*", features[,2])
featuresWanted.names <- features[featuresWanted,2]
featuresWanted.names = gsub('-mean', 'Mean', featuresWanted.names)
featuresWanted.names = gsub('-std', 'Std', featuresWanted.names)
featuresWanted.names <- gsub('[-()]', '', featuresWanted.names)
# Load the datasets
train <- read.table("UCI HAR Dataset/train/X_train.txt")[featuresWanted]
trainActivities <- read.table("UCI HAR Dataset/train/Y_train.txt")
trainSubjects <- read.table("UCI HAR Dataset/train/subject_train.txt")
train <- cbind(trainSubjects, trainActivities, train)
test <- read.table("UCI HAR Dataset/test/X_test.txt")[featuresWanted]
testActivities <- read.table("UCI HAR Dataset/test/Y_test.txt")
testSubjects <- read.table("UCI HAR Dataset/test/subject_test.txt")
test <- cbind(testSubjects, testActivities, test)
# merge datasets and add labels
allData <- rbind(train, test)
colnames(allData) <- c("subject", "activity", featuresWanted.names)
# turn activities & subjects into factors
allData$activity <- factor(allData$activity, levels = activityLabels[,1], labels = activityLabels[,2])
allData$subject <- as.factor(allData$subject)
allData.melted <- melt(allData, id = c("subject", "activity"))
allData.mean <- dcast(allData.melted, subject + activity ~ variable, mean)
write.table(allData.mean, "tidy.txt", row.names = FALSE, quote = FALSE)
# create a second, independent data set with the average of each variable for each activity and each subject
averageData<-aggregate(. ~subject + activity, allData, mean)
averageData<-averageData[order(averageData$subject,averageData$activity),]
write.table(averageData, file = "averagetidydata.txt",row.name=FALSE)
install.packages("knitr)
""
install.packages("knitr")
library(reshape2)
library(plyr)
# Load activity labels + features
activityLabels <- read.table("UCI HAR Dataset/activity_labels.txt")
activityLabels[,2] <- as.character(activityLabels[,2])
features <- read.table("UCI HAR Dataset/features.txt")
features[,2] <- as.character(features[,2])
# Extract only the data on mean and standard deviation
featuresWanted <- grep(".*mean.*|.*std.*", features[,2])
featuresWanted.names <- features[featuresWanted,2]
featuresWanted.names = gsub('-mean', 'Mean', featuresWanted.names)
featuresWanted.names = gsub('-std', 'Std', featuresWanted.names)
featuresWanted.names <- gsub('[-()]', '', featuresWanted.names)
# Load the datasets
train <- read.table("UCI HAR Dataset/train/X_train.txt")[featuresWanted]
trainActivities <- read.table("UCI HAR Dataset/train/Y_train.txt")
trainSubjects <- read.table("UCI HAR Dataset/train/subject_train.txt")
train <- cbind(trainSubjects, trainActivities, train)
test <- read.table("UCI HAR Dataset/test/X_test.txt")[featuresWanted]
testActivities <- read.table("UCI HAR Dataset/test/Y_test.txt")
testSubjects <- read.table("UCI HAR Dataset/test/subject_test.txt")
test <- cbind(testSubjects, testActivities, test)
# merge datasets and add labels
allData <- rbind(train, test)
colnames(allData) <- c("subject", "activity", featuresWanted.names)
# turn activities & subjects into factors
allData$activity <- factor(allData$activity, levels = activityLabels[,1], labels = activityLabels[,2])
allData$subject <- as.factor(allData$subject)
allData.melted <- melt(allData, id = c("subject", "activity"))
allData.mean <- dcast(allData.melted, subject + activity ~ variable, mean)
write.table(allData.mean, "tidy.txt", row.names = FALSE, quote = FALSE)
# create a second, independent data set with the average of each variable for each activity and each subject
averageData<-aggregate(. ~subject + activity, allData, mean)
averageData<-averageData[order(averageData$subject,averageData$activity),]
write.table(averageData, file = "averagetidydata.txt",row.name=FALSE)
knit2htl(Codebook.md)
knit2html(Codebook.md)
knitr::knit2html(Codebook.md)
knitr::knit2html(Codebook.Rmd)
library(reshape2)
# Load activity labels + features
activityLabels <- read.table("UCI HAR Dataset/activity_labels.txt")
activityLabels[,2] <- as.character(activityLabels[,2])
features <- read.table("UCI HAR Dataset/features.txt")
features[,2] <- as.character(features[,2])
# Extract only the data on mean and standard deviation
featuresWanted <- grep(".*mean.*|.*std.*", features[,2])
featuresWanted.names <- features[featuresWanted,2]
featuresWanted.names = gsub('-mean', 'Mean', featuresWanted.names)
featuresWanted.names = gsub('-std', 'Std', featuresWanted.names)
featuresWanted.names <- gsub('[-()]', '', featuresWanted.names)
# Load the datasets
train <- read.table("UCI HAR Dataset/train/X_train.txt")[featuresWanted]
trainActivities <- read.table("UCI HAR Dataset/train/Y_train.txt")
trainSubjects <- read.table("UCI HAR Dataset/train/subject_train.txt")
train <- cbind(trainSubjects, trainActivities, train)
test <- read.table("UCI HAR Dataset/test/X_test.txt")[featuresWanted]
testActivities <- read.table("UCI HAR Dataset/test/Y_test.txt")
testSubjects <- read.table("UCI HAR Dataset/test/subject_test.txt")
test <- cbind(testSubjects, testActivities, test)
# merge datasets and add labels
allData <- rbind(train, test)
colnames(allData) <- c("subject", "activity", featuresWanted.names)
# turn activities & subjects into factors
allData$activity <- factor(allData$activity, levels = activityLabels[,1], labels = activityLabels[,2])
allData$subject <- as.factor(allData$subject)
allData.melted <- melt(allData, id = c("subject", "activity"))
allData.mean <- dcast(allData.melted, subject + activity ~ variable, mean)
write.table(allData.mean, "tidy.txt", row.names = FALSE, quote = FALSE)
test <- grep(".*mean.*|.*std.*",features[,2])
str(test)
library(lattice)
lines()
lpoints()
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
prin(p)
print(p)
plot(p)
library(datasets)
data(airquality)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot)
install.packages(ggplot2)
install.packages("ggplot2")
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
set.seed(1234)
par(mar=c(0,0,0,0))
x <- rnorm(12,mean=rep(1:3))
x <- rnorm(12,mean=rep(1:3),each=4),sd=0.2)
x <- rnorm(12,mean : rep(1:3,each=4),sd=0.2)
x <- rnorm(12,mean = rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean = rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col='blue',pch=19,cex=2)
text(x+ 0.05,y+ 0.05,labels=as.character(1:12))
DataFrame <- data.frame(x,y)
KmeansObj <- kmeans(DataFrame, centers=3)
names(KmeansObj)
par(mar= rep(0.2,4))
plot(x,y,col=KmeansObj$cluster,pch=19,cex=2)
points(KmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
getwd()
setwd("C:/Users/pedew_000/RepData_PeerAssessment1/")
setwd("C:/Users/pedew_000/RepData_PeerAssessment1/")
Activity_Monitoring <- read.csv("activity.csv", header = TRUE, sep =",",
colClasses = c("numeric", "character", "numeric"))
head(Activity_Monitoring)
setwd("C:/Users/pedew_000/RepData_PeerAssessment1/")
Activity_Monitoring <- read.csv(unz("activity.zip","activity.csv"), header = TRUE, sep =",",
colClasses = c("numeric", "character", "numeric"))
head(Activity_Monitoring)
